{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the `teeth.array` module: explore and tokenize text\n",
    "\n",
    "## intro\n",
    "\n",
    "text is deep. its size is epic. its rules are murky. it summons things from across timespace. it has to be chaotic, so everything can fit.\n",
    "\n",
    "even just splitting text into tokens is hard. there are rules, but those almost always have exceptions, and the exceptions tend to multiply. it's helpful to experiment and adjust, in small starts.\n",
    "\n",
    "these data structures are intended as a tool to do just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *technical detail\n",
    "\n",
    "this notebook is bundled with source; we need to make sure the cells below can import from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "package_path = os.path.abspath( '..' )\n",
    "\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append( package_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try this out on a novel borrowed from the public domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_path = 'moby_dick.txt'\n",
    "\n",
    "with open( raw_text_path, 'r') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[ 0 : 27 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `str` returned from the file read is the starting point for our new datastructure. let's use it to create a new instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teeth.array import TextStrata\n",
    "\n",
    "t = TextStrata( raw_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initially, a `TextStrata` exposes indices and slices just like the underlying string. the smallest tokens are characters, and slices are just subsequences of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[ 0 : 27 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a common next step would be to tokenize the text into words. if we can define which strings are not words, `TextStrata` will construct a split of the string that distinguishes words from separators.\n",
    "\n",
    "here's a first attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teeth.array import split\n",
    "from teeth.pattern import matches\n",
    "\n",
    "def not_a_word( x ):\n",
    "    return x in ' \\n'\n",
    "\n",
    "with split( not_a_word, t ) as words:\n",
    "    print( words[ 0 : 7 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the `matches` function takes a regular expression and returns a bool-valued function that returns true if a string matches.\n",
    "- the `split` expression takes the predicte in its first argument and uses it to separate values in the `TextStrata` argument into tokens and delimiters.\n",
    "- the split is scoped to the `with` statement; the underlying value of `t` does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that split is not quite clean. the whitespace has been separated, but not the punctuation. that's easy to fix! all we have to do is adjust the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_a_word( x ):\n",
    "    return x in ' \\n;,.!?'\n",
    "\n",
    "with split( not_a_word, t ) as words:\n",
    "    print( words[ 0 : 7 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
