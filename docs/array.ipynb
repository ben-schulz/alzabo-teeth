{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the `teeth.array` module: explore and tokenize text\n",
    "\n",
    "## intro\n",
    "\n",
    "text is deep. its size is epic. its rules are murky. it summons things from across timespace. it has to be chaotic, so everything can fit.\n",
    "\n",
    "even just splitting text into tokens is hard. there are rules, but those almost always have exceptions, and the exceptions tend to multiply. it's helpful to experiment and adjust, in small starts.\n",
    "\n",
    "these data structures are intended as a tool to do just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *technical detail\n",
    "\n",
    "this notebook is bundled with source; we need to make sure the cells below can import from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "package_path = os.path.abspath( '..' )\n",
    "\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append( package_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try this out on a novel borrowed from the public domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_path = 'moby_dick.txt'\n",
    "\n",
    "with open( raw_text_path, 'r') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[ 0 : 27 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `str` returned from the file read is the starting point for our new datastructure. let's use it to create a new instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teeth.array import TextStrata\n",
    "\n",
    "t = TextStrata( raw_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initially, a `TextStrata` exposes indices and slices just like the underlying string. the smallest tokens are characters, and slices are just subsequences of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[ 0 : 27 ]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
