{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the `teeth.array` module: explore and tokenize text\n",
    "\n",
    "## intro\n",
    "\n",
    "text is deep. its size is epic. its rules are murky. it summons things from across timespace. it has to be chaotic, so everything can fit.\n",
    "\n",
    "even just splitting text into tokens is hard. there are rules, but those almost always have exceptions, and the exceptions tend to multiply. it's helpful to experiment and adjust, in small starts.\n",
    "\n",
    "these data structures are intended as a tool to do just that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *technical detail\n",
    "\n",
    "this notebook is bundled with source; we need to make sure the cells below can import from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "package_path = os.path.abspath( '..' )\n",
    "\n",
    "if package_path not in sys.path:\n",
    "    sys.path.append( package_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's try this out on a novel borrowed from the public domain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text_path = 'moby_dick.txt'\n",
    "\n",
    "with open( raw_text_path, 'r') as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[ 0 : 27 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `str` returned from the file read is the starting point for our new datastructure. let's use it to create a new instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teeth.array import TextStrata\n",
    "\n",
    "t = TextStrata( raw_text )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initially, a `TextStrata` exposes indices and slices just like the underlying string. the smallest tokens are characters, and slices are just subsequences of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[ 0 : 27 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a common next step would be to tokenize the text into words. if we can define which strings are not words, `TextStrata` will construct a split of the string that distinguishes words from separators.\n",
    "\n",
    "here's a first attempt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teeth.array import split\n",
    "\n",
    "def not_a_word( x ):\n",
    "    return x in ' \\n'\n",
    "\n",
    "with split( not_a_word, t ) as words:\n",
    "    print( words[ 0 : 7 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the `split` expression takes a predicate as its first argument and an instance of `TextStrata` as it second.\n",
    "- any token for which the predicate returns true will be identified as a delimiter.\n",
    "- indexing and slicing works similar to a normal `list`, both before and after the split\n",
    "- within the scope of the split, ordinal indices identify tokens induced by the predicate.\n",
    "- the split is scoped to the `with` statement; the underlying value of `t` does not change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that split is not quite clean. the whitespace has been separated, but not the punctuation. that's easy to fix! all we have to do is adjust the pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def not_a_word( x ):\n",
    "    return x in ' \\n;,.!?'\n",
    "\n",
    "with split( not_a_word, t ) as words:\n",
    "    print( words[ 0 : 7 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note again that the underlying contents of `t` don't change outside the scope of the `with`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( t[ 0 : 7 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this behavior is quite useful when doing exploratory work in a notebook, since it prevents accidentally changing the underlying data when cells are executed multiple times, or out of order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## persistent tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting the same text over and over will be computationally expensive, requiring at least `O(n)` time in the length of the text. once the right split has been worked out, it would be helpful to make it persistent. the `TextStrata` object exposes a method for doing so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.split_where( not_a_word )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now, subsequent slices into `t` will reference tokens generated by the split, instead of the underlying characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( t[ 0 : 7 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( t[ 111198 : 111256 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`t` is also iterable by token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = iter( t )\n",
    "for _ in range( 7 ):\n",
    "    print( next( token ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## layered token splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in many nlp tasks, we'll be interested in higher-order tokens, i.e. tokens comprised of other tokens. for example, documents are often represented as sequences of sentences, which are in turn sequences of words.\n",
    "\n",
    "the interface exposed by `TextStrata` generalizes seamlessly to these tasks. all we have to do is define a predicate for the new split. let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sentence_ends( x ):\n",
    "    return 0 < len( x ) and x[ 0 ][ 0 ] in '.?!'\n",
    "\n",
    "with split( sentence_ends, t ) as sentences:\n",
    "    for ix in range( 10050, 10060 ):\n",
    "        print( sentences[ ix ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the underlying layers are still accessible, using the `layer` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from teeth.array import layer\n",
    "\n",
    "with layer( 0, t ) as char_level:\n",
    "    print( char_level[ 0 : 27 ] )\n",
    "    \n",
    "with layer( 1, t ) as word_level:\n",
    "    print( word_level[ 0 : 7 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layers are counted upward; `layer( 0, t )` exposes the underlying iterable of characters, while `layer( 1, t )` exposes the word-level split added by `split_where`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
